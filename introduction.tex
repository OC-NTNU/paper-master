\section{Introduction}

Information extraction is the task of extracting structured information from natural language text. Historically, information extraction research focused on named entity recognition (NER), with research effort slowly turning to more complex tasks such as relation or event extraction when state-of-the-art on NER had reached acceptable levels. 

When developing information extraction systems for a new domain, the development effort normally adheres to an incremental approach similar to historical development of the information extraction field, starting with named entity recognition, and subsequently working on relation or event extraction. The architecture of most state-of-the-art information extraction systems reflect this development pattern, as they are pipeline systems with first a NER component, followed by a relation/event extraction component, where the output of the first component is provided as input to the second component.

The drawback of a pipeline architecture is error propagation, as each component makes hard choices that cannot be undone by downstream components, even though new information may surface later in the extraction process. Some recent research effort has therefore focused on joint extraction architectures, where steps are conducted in parallel.\todo{citations?} However, joint extraction architectures have not reached the level of maturity required for state-of-the-art performance, and have other drawbacks, such as the increased complexity of learning a joint probability distribution, that make them infeasible for certain circumstances\todo{Am I just making this stuff up, or is it true?}, such as when training data is scarce, and the lack of off-the-shelf joint information extraction systems. 

The traditional pipeline architecture for information extraction appears to have arisen not as a principled design decision, but rather accidentally by following the research effort. This paper therefore investigates another approach to the pipeline architecture, where events detection precedes entity detection, and shows that for certain extraction tasks, reversing the order of the pipeline can prove beneficial.  

\subsection{Extraction task}

Progress in research towards understanding the full range of causes and consequences of global warming is hampered by the wide range of relevant disciplines, which include, among others, climate science, earth science, oceanography, ecology and biochemistry. Text mining can be used to provide an overview of the research in the relevant disciplines, and help the researchers draw parallels or uncover hidden connections. As a first step towards a discovery support system in climate change science, an information extraction system is being developed.

To extract relations that are general enough to be useful across disciplines, but still specific enough to be useful for a researcher, the information extraction system targets the extraction of events where quantiative variables undergo a directional change, such as \emph{increase in atmospheric CO$_2$}, and interactions between such change events, which are restricted to causal and correlative relations. A pilot corpus consisting of 10 scientific journal articles has been annotated, as described in \citet{mar14}. The remainder of this section provides a sufficient introduction to the annotation scheme as to understand the specifics of the experiment.

The trigger categories that were used to annotate text spans of interest in the corpus are presented in Table \ref{ann_scheme}.

\begin{figure}
\Tree[.CATEGORY [.ENTITY \textit{Variable} \textit{Thing} ]
          [.EVENT [.\textit{Change} \textit{Increase} \textit{Decrease} ] 
         		  [.INTERACTION \textit{Cause} \textit{Correlate} ] 
          ] 
     ]
\caption{Type Hierarchy for the Annotation Scheme}
\label{ann_scheme}
\end{figure}

\emph{Variable} in our annotation scheme is defined as a \emph{quantitative variable}, meaning an entity than can be measured and assigned some value along some ordered axis, either as a numerical value or a value from a totally ordered set of discrete states. This includes among other counts, frequencies and ratios. Text spans that can be interpreted as quantitative variables out of context are abundant in the scientific literature, but only a small subset of these are of interest. Only variables that occur in an event are therefore annotated in the corpus. 

\emph{Thing} is used to annotate any span of text that functions as the argument of a change event, while not fulfilling the requirements to be annotated as a variable. 

A change event describes a directional, quantitative change in the value of a quantitative variable. \emph{Increase} is used to annotate a change in the positive direction, \emph{decrease} annotates changes in the negative direction and \emph{change} is used when the direction is underspecified in the text. All the change events take an entity as the \emph{theme} argument, signalling the variable that undergoes the change.

Interaction categories mark text spans that explicitly signal interactions between two change events. The types of interactions that are used in the corpus are \emph{cause} and \emph{correlate}. Cause events take two arguments: An \emph{agent}, which marks the change event that causes the other change event to come about. The \emph{theme} argument is used for the caused change event. Correlation events also take two arguments, the \emph{theme} and \emph{co-theme}. It was noticed during corpus annotation that there is a tendency in natural language to focus on one of the changes as initiating the other, giving such correlations a directional interpretation. For correlations with an directional interpretation, the initiating event is marked as the theme, and the other event is marked as the co-theme. If the correlation has no directional interpretation, the event that is the syntactic object of the correlation trigger is marked as co-theme, and the other event as theme by default.

Three additional categories are used in the corpus to handle some common language constructs, namely conjunction, disjunction and referring expressions. The categories \emph{and} and \emph{or} handle conjunctions and disjunctions, respectively, and each take two \emph{part} arguments. Referring expressions are handled by the \emph{RefExp} categories, which takes the antecedent as the \emph{coref} argument.

\subsection{Pipeline Architecture}

The canonical information extraction pipeline architecture uses a bottom-up approach, in which the lowest order categories (entities) are extracted first, and subsequently used as evidence during later steps when higher order categories (events) are extracted. However, it is also possible to conceive a top-down approach, where higher order categories are extracted first, and the lower level entities are extracted later. 

It was mentioned in the introduction that the bottom-up pipeline approach did not arise as a consequence of an informed design decision, but rather occurred accidentally due to the way research had been conducted. One hypothesis is that the bottom-up approach is successful for many extraction tasks because entity extraction is, in many tasks, a simpler sub-task than event extraction. Starting extraction from the simplest categories makes sense, as the performance on these categories is likely to be high, and the system therefore able to produce trustworthy information that can be used as supporting evidence during extraction of more difficult categories. A bolder hypothesis is that a pipeline approach will yield the best results if starting extraction from the simplest category group.
