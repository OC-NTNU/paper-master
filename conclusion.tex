\section{Discussion and conclusion}

It is likely that much of the success of the alternative pipeline is due to the fact that it started by extracting from change events, which appear to be the easiest categories. Evidence found during this step facilitates extraction of the more difficult categories. This can be contrasted with the traditional pipeline system, which started from the quite difficult entity categories, and the error-prone evidence found the that step could not improve performance on the already quite easy events categories. This result may possibly be generalized to the hypothesis that a pipeline information extraction system will attain the best results by starting extraction with the easiest category group.

The hypothesis is supported by the following probability theoretic argument: Information extraction can be formulated finding $argmax_{\boldsymbol{X}}\ P(X_0, X_1, ..., X_n | Y)$, where $X_0 ... X_n$ are the extraction item classes (entities, events and relations). Because it is ofen infeasable to finding argmax over the joint probablity distibution, the pipeline approach approximates the global optimum by decomposing the joint distribution using the chain rule of probability $P(X_0, X_1 ... X_n | Y) = P(X_0 | Y)P(X_1 | X_0, Y)P(X_2 | X_1, X_0, Y) \dots P(X_n | X_{n-1}, ..., X_0, Y)$ and greedily optimizing each step in the chain, thus computing $\hat{X_0} = argmax_{X_0}\ P(X_0 | Y)$, $\hat{X_1} = argmax_{X_1}\ P(X_1 | \hat{X_0}, Y)$ ... . In this setup, any error in $X_0$ will aversely affect all subsequent steps, as they then will try to optimize based on a suboptimal variable assignment. On the other hand, an error in $X_n$Â will only affect the step itself. To minimize the approximation error, one should put the item classes that are least error-prone and least dependent on information from the other item classes first.

Verifying the hypothesis empirically, and exploring other heuristics for determining the optimal pipeline architecture for an extraction task remain tasks for future research. The development of an information extraction system that can lets the user make arbitrary adjustments to the pipeline would make this line of research immediately applicable. A longer term goal is the development of a system that is able to automatically select the optimal pipeline based directly on the data.

Running the extaction experiments has also given insight into the strengths and weaknesses of our annotation scheme, and refining the annotation scheme to sort out unnecessary complexities also remains an important research task. For instance, experiments in reasoning have uncovered that it does not make sense to make a binary distinction between entities that require no semantic interpretation (\emph{variables}) and entities that require full semantic interpretation (\emph{things}), because entities always require semantic interpretation to a certain degree. As an example, consider \emph{growth} in the example sentence "Increasing concentrations of CO2 cause  a strong decline in growth...". \emph{Growth} is arguably a quantitative variable, but for this to be useful during reasoning, a semantic interpretation component must disambiguate further what thing in the real world that the growth pertains to. In future versions of the annotation scheme, the category \emph{thing} will therefore be deprecated, and the category \emph{variable} used in all cases. All entities will then be passed to the semantic interpretation module.

THERE IS PLENTY OF FUTURE WORK THAT WE CAN MENTION HERE, AMONG OTHERS ONTOLOGIES, ADDING CO-REF TREATMENT, VARIABLE DISAMBIGUATION, REASONING, LDB ETC. IS IT WORTH MENTIONING, OR JUST NOISE FOR THE READER? 
